# -*- coding: utf-8 -*-
"""RBL_20924005.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HmM1YRaN351Cm9YExJexdQU91se1MMMI

# **A. Latar Belakang**

Kesenjangan antara suplai dan demand menjadi salah satu faktor penting dan juga tantangan utama dalam manajemen rantai pasokan. Ketidakseimbangan pada suplai dan demand dapat menyebabkan berbagai masalah seperti kelebihan stok, kekurangan stok, dan meningkatnya biaya operasional. Hal ini akan berdampak pada pendapatan dan margin keuntungan perusahaan. Selain dipengaruhi oleh permintaan pasar, kuantitas produksi juga dipengaruhi oleh harga bahan baku. Bahan baku, seperti timah, menjadi salah satu komponen utama yang menyusun biaya produksi. Harga bahan baku dapat mempengaruhi kuantitas produksi dan margin keuntungan. Dengan anggaran yang sama, harga bahan baku yang rendah dapat meningkatkan kuantitas bahan baku yang diperoleh sehingga dapat meningkatkan kuantitas produks dan meningkatkan margin keuntungan. Terdapat kasus ketika perusahaan membeli bahan baku dengan kuantitas tinggi dan harga tinggi tetapi dijual saat harga produk sedang turun. Hal ini akan menyebabkan berkurangnya margin keuntungan bahkan menyebabkan kerugian bagi perusahaan. Selain itu, kejutan eksternal seperti Covid-19 dan krisis moneter menjadi salah satu faktor yang berdampak pada pasar. Oleh karena itu, penting bagi perusahaan untuk memprediksi harga bahan baku dan melakukan pembelian pada waktu yang tepat. Penelitian ini bertujuan untuk memprediksi harga bahan baku timah yang robust terhadap kejutan eksternal sehingga dapat membantu perusahaan produsen berbahan baku timah, Perusahaan ABC, untuk terhindar dari kerugian bahkan meningkatkan margin keuntungan. Metode dan prediktor merupakan komponen yang harus dipertimbangkan dalam memodelkan harga timah. Pada penelitian ini akan menggunakan metode Artificial Neural network untuk memprediksi harga timah. Multi-prediktor yang akan digunakan yaitu data historis harga timah, nilai tukar mata uang, harga minyak dunia, harga batu bara, dan harga minyak mentah. Dengan penelitian ini diharapkan dapat memprediksi harga timah dengan akurasi yang tinggi dan robust terhadap efek kejutan eksternal. Hasil prediksinya dapat mengatasi kecemasan produsen dalam menentukan keputusan dalam pembelian bahan baku. Hasil ini akan digunakan sebagai salah satu bagian dari bahan tesis, tentu akan ditingkatkan semaksimal mungkin untuk akurasinya.

Data yang digunakan
- [Nilai Tukar](https://id.investing.com/currencies/usd-idr-historical-data)
- [Harga Timah]( https://id.investing.com/commodities/tin)
- [Harga Batu barat](https://id.investing.com/commodities/newcastle-coal-futures )
- [Harga Minyak](https://id.investing.com/commodities/crude-oil )

## **Instalasi Package**
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/content/gdrive')
# %cd '/content/gdrive/My Drive/Magister/Semester 1/Metode Penelitian/Tesis Timah'

!pip install matplotlib==3.8.1 seaborn

pip install scikeras

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import random
from itertools import product
import seaborn as sns
from datetime import timedelta, datetime
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.model_selection import GridSearchCV
from scikeras.wrappers import KerasRegressor
import tensorflow as tf
import os

"""# **B. Data Preprocessing**

## **B.1.Data Parsing dan Data Cleaning**
"""

name_ = []
for name in os.listdir():
  if name.endswith('csv'):
    name_.append(name)
    print(name)
    data=pd.read_csv(name, sep=';')
    print(len(data))
    display(pd.concat([data.head(2),data.tail(2)]))
    print(data.info())

"""Data yang akan digunakan yaitu feature harga terakhir pada setiap data harga. Namun data yang kami miliki masih bertipe `object`, perhatikan bahwa untuk data dengan format penulisan inggris menggunakan `.` sebagai pemisah ribuan dan `,` sebagai pemisah desimal. Sedangkan untuk format Indonesia menggunakan pemisah `,` sebagai ribuan dan menggunakan `.` sebagai pemisah desimal. Oleh karena itu, perlu mengubah data menjadi format `float` dengan pemisah desimal menggunakan `.` dan tanpa pemisah desimal. Selain itu, feature waktu tiap data masih beragaam dengan
- `Minyak Mentah`: `8-01-1990` s.d `19-11-2024`
- `Batu Bara` : `8-12-2008` s.d `18-11-2024`
- `Timah` : `7-7-2008` s.d `15-11-2024`
- `Currency`: `19-11-1990` s.d `19-11-2024`

Perbedaan data ini disebabkan oleh masing-masing index yang digunakan untuk mengenerate harga tersebut baru ada pada tanggal tersebut. Karena data yang kan digunakan harus pada tanggal yang sama maka saya akan mengambil data pada feature tanggal dari `8-12-2008` s.d `15-11-2024`. Berikut merupakan code untuk mengubah `type` data, menggabungkan bedasarkan harga terakhir dan juga menyamakan tanggal nya.
"""

for i, val in enumerate(name_):
  if 'USD' in val:
    cur_ = name_.pop(i)
    name_.insert(0,cur_)

name_ = [name_[0]]+name_[2:5]

Time_ = ['Tanggal','Date']
Harga_ = ['Terakhir','Price']
data=pd.read_csv(name_[0], sep=';')
data_ = data.iloc[:,:2]
data_.columns = ['Tanggal','Currency']
if data.columns[0]=='Date':
  data_['Tanggal']=pd.to_datetime(data_['Tanggal'], format='%m/%d/%Y')
  data_['Currency']=data_['Currency'].str.replace(',','').astype('float')
else:
  data_['Tanggal']=pd.to_datetime(data_['Tanggal'], format='%d/%m/%Y')
  data_['Currency']=data_['Currency'].str.replace('.','', regex=False).str.replace(',','.', regex=False).astype('float')
for _ in name_[1:]:
  data = pd.read_csv(_, sep=';')
  for j in Time_:
    if j in data.columns:
      for k in Harga_:
        if k in data.columns:

          if j=='Tanggal':
            data[j]=pd.to_datetime(data[j], format= '%d/%m/%Y')
            data[k]=data[k].str.replace('.','', regex=False).str.replace(',','.', regex=False).astype('float')
          else:
            data[j]=pd.to_datetime(data[j], format='%m/%d/%Y')
            data[k]=data[k].str.replace(',','').astype('float')
          Data_ = data[[j,k]]
          name = _.split()
          Data_.columns = ['Tanggal', f'Harga {" ".join(name[2:-1])}']
          data_ = data_.merge(Data_, on='Tanggal', how='inner')# suffixes=('',f' {" ".join(name[2:-1])}'))

pd.concat([data_.head(2),data_.sample(2),data_.tail(2)])

"""## **B.2. Feature Engineering**

Untuk feature `tanggal` masih dalam `type` `datetime` sehingga masih belum bisa digunakan sebagai variabel penjelas tetapi feature ini bisa digali dan digunakan untuk tiap bagiannya: `tahun`, `bulan`,dan `hari`. Kemudian ketiga feture ini akan ditambahkan ke variabel $X$ sebagai calon variabel penjelas.
"""

#data_['Tahun']=data_.Tanggal.dt.year
data_['Bulan']=data_.Tanggal.dt.month
data_['Hari']=data_.Tanggal.dt.day

data_['Tanggal'].min()

"""## **B.3. Initial EDA**"""

pd.concat([data_.head(2),data_.sample(2),data_.tail(2)])[data_.columns[1:]]

data_.info()

"""Data yang akan digunakan sudah sesuai format `type` nya. Dengan fetures `Tanggal`, `Currency`,`Minyak Mentah`,`Batu Bara`, dan `Timah`. Berikut merupakan statistik dari data yang akan digunakan."""

data_[data_.columns[1:]].describe()

"""Berdasarkan hasil di atas, terdapat `3869` data point dengan waktu dari `8-12-2009` s.d `15-11-2024`. Pada periode tersebut, data harga `Timah` memiliki rentang harga `9870` s.d `48650` dengan rata-rata `21941` dan standar deviasi `6172`. Sedangkan statistik untuk feture lainnya dapat diliha pada tabel di atas. Hal menarik yang terdapat pada tabel di atas yaitu harga minimal dari `Minyak Mentah` sebesar `-37.6` Dolar per barel pada tanggal `20-04-2020`."""

data_minyak = data_.iloc[:,2]
data_[data_.iloc[:,2]<0]

data_.iloc[1109:1112,:]

"""Setelah ditelusuri, data ini memang benar adanya, berikut kronologinya.

Penjelasan Kejadian

  - `19 April 2020`: Harga WTI berada di kisaran sekitar $\$10$ per barel. Penurunan ini disebabkan oleh permintaan minyak yang sangat rendah akibat pandemi COVID-19 dan pembatasan perjalanan global.
  - `20 April 2020`: Harga WTI anjlok hingga $-37,63$ dolar per barel. Ini terjadi karena kontrak minyak untuk pengiriman Mei 2020 akan segera berakhir, dan para pedagang berusaha menghindari mengambil pengiriman fisik minyak mentah.

Mengapa Harga Bisa Negatif?

  - Permintaan Rendah: Pandemi menyebabkan permintaan minyak global turun drastis, sementara produksi terus berjalan.

  - Kapasitas Penyimpanan Penuh: Penyimpanan minyak di pusat distribusi utama seperti Cushing, Oklahoma, hampir penuh. Tidak ada tempat lagi untuk menyimpan minyak mentah.

  - Kontrak Berjangka: Banyak pedagang tidak ingin menerima pengiriman fisik minyak dan bersedia membayar pembeli untuk mengambil kontrak dari tangan mereka.

  ChatGPT

**Visualisasi Data**

Berikut akan dilakukan visualisasi menggunakan data bulanan, yaitu data harga terakhir pada tanggal `1`, `2`, dan `3`. Karena terdapat libur
"""

#monthly Immediate
filtered_immediate = data_[data_['Tanggal'].dt.day.isin([1,2,3])]
data_immediate = filtered_immediate.groupby([filtered_immediate['Tanggal'].dt.year, filtered_immediate['Tanggal'].dt.month]).first().reset_index(drop='True')
#data_immediate.iloc[:, 1:][col_2].plot()

col = data_.columns
x=data_immediate.iloc[:,0]
color_ = ['b','y','g','r','k','m','c']
for i,col_name in enumerate(col[1:]):
  fig, ax1 = plt.subplots()

  # Plot untuk y-axis pertama
  ax1.plot(x, data_immediate[col_name], f'{color_[i]}-', label=f'{col_name}')
  ax1.set_xlabel('Time')       # Label untuk sumbu x
  ax1.set_ylabel(f'{col_name}', color=color_[i])  # Label untuk y-axis pertama
  ax1.tick_params(axis='y', labelcolor=color_[i])  # Warna label y1

  # Membuat axis kedua
  ax2 = ax1.twinx()  # Membuat axis y kedua berbagi sumbu x
  ax2.plot(x, data_immediate.iloc[:,4], 'r--', label=f'{col[4]}')
  ax2.set_ylabel(f'{col[4]}', color='r')  # Label untuk y-axis kedua
  ax2.tick_params(axis='y', labelcolor='r')  # Warna label y2

  # Tambahkan legend
  fig.legend(loc="upper left", bbox_to_anchor=(0.1, 0.9))

  # Tampilkan plot
  plt.title(f"{col_name} vs {col[4]}")
  plt.show()

plt.figure(figsize=(10, 8))
corr_=data_.iloc[:,1:-2].corr()
mask = np.triu(np.ones_like(corr_, dtype='bool'),k=1)
sns.heatmap(corr_,
    mask = mask,
    annot=True,                  # Menampilkan nilai korelasi
    fmt=".2f",                   # Format angka desimal
    cmap="coolwarm",             # Warna heatmap
    cbar=True,                   # Menampilkan color bar
    cbar_kws={"shrink": 0.8},    # Ukuran color bar
    linewidths=0.5,              # Lebar garis antar kotak
    linecolor=None,           # Warna garis antar kotak
    vmin=-1, vmax=1,             # Batas minimum dan maksimum nilai
    square=True,                 # Kotak berbentuk persegi
    xticklabels=True,            # Menampilkan label x
    yticklabels=True             # Menampilkan label y
)

# Tambahkan judul
plt.title("Heatmap Korelasi", fontsize=16)

# Tampilkan heatmap
plt.show()

df = data_
df["Tanggal"] = pd.to_datetime(df["Tanggal"])

# Menyusun y: hanya 'Tanggal' dan 'Harga Timah Berjangka'
y_list = []
X_list = []

for index, row in df.iterrows():
    target_date = row["Tanggal"] - timedelta(days=30)  # Target awal 30 hari sebelumnya

    # Cari tanggal dalam rentang 30-32 hari sebelumnya
    x_row = df[(df["Tanggal"] >= target_date) & (df["Tanggal"] <= target_date + timedelta(days=2))]

    if not x_row.empty:
        x_selected = x_row.iloc[-1]  # Pilih tanggal terbaru dalam rentang
        X_list.append(x_selected)
        y_list.append(row)  # Hanya menambahkan ke y jika ada pasangan di X

# Konversi list ke DataFrame
X = pd.DataFrame(X_list).reset_index(drop=True)
y = pd.DataFrame(y_list)[["Tanggal", "Harga Timah Berjangka"]].reset_index(drop=True)

# Menampilkan hasil
print("Ukuran X:", X.shape)
print("Ukuran y:", y.shape)

print("\nX:")
display(X)

print("\ny:")
display(y)

"""## **B.4. Data**

Pada subbagian ini akan memproses data lebih lanjut sehingga data siap digunakan. Proses yang dilakukan yaitu menyesuaikan periode waktu antara variabel penjelas dengan target, melakukan *labeling* pada variabel target, dan melakukan *scaling* pada data.

Perlu adanya penyesuaian periode data antara $X$ dan $y$ yang akan digunakan. Hal ini, disebabkan oleh, pemodelan yang akan di lakukan yaitu data pada waktu $T$ akan pada $X$ akan digunakan untuk memodelkan $y$ pada waktu $T+k$ dengan $k$ merupakan rentang waktu prediksi. Sebagai contoh kita ingin memprediksi harga timah pada bulan berikutnya maka $k=30$ dengan asumsi satu bulan $30$ hari.

Langkah selanjutnya yaitu pemberian label pada variabel $y$. Karena pemodelan kali ini ingin melakukan klasifikasi dan juga regresi maka kita memiliki dua target ($Y$). Untuk pemberian label akan digunakan tingkat perubahan harga pada waktu ke-$T+k$ terhadap waktu ke-$t$. Misal $R_t$ merupakan tingkat perubahan harga pada waktu ke-$t$ maka
\begin{align}
R_{t+k}=\dfrac{P_{t+k}-P_t}{P_t}
\end{align}

Dengan $P_t$ merupakan harga timah pada waktu ke-$t$, kemudian $R_{t+k}$ digeser ke periode waktu $t$ pada variabel $X$ sehingga diperoleh satu label untuk regresi yaitu $R_{t}$. Kemudian, pemberian label untuk klasifikasi menggunakan formula berikut.
\begin{align}
S_t = \begin{cases} 1 \ , \ & \ R_t\ge 0 \\
0 \ , \ & \ R_t<0
\end{cases}
\end{align}
Dengan $S_t$ menunjukan status harga timah pada waktu $t+k$ apakah `naik` dengan label `1` atau `turun` dengan label `0`. Kita Memperoleh label `$R_t$` dan `$S_t$` yang kemudian akan digunakan sebagai target regresi dan klasifikasi secara berturut-turut.

Setelah itu, akan dilakukan *scaling* pada $X$ maupun $y$, hal ini penting untuk dilakukan agar semua variabel memiliki porsi yang sama dan tidak ada dominasi dalam klasifikasi maupun regresi. Penyekalaan yang akan digunakan yaitu `min max scaling` agar semua variabel berada pada rentang nilai `[0,1]`. Namun untuk variabel $R_t$ perlu melakukan modifikasi pada formula `min max scaling`. Hal ini dilakukan agar hasil dari regresi memungkinkan lebih besar maupun lebih kecil berturut-turut dari nilai `max-min` data historis. Fprmula yang digunakan yaitu

\begin{align}
DataScaled_i = \dfrac{data_i-min(data)}{\alpha(max(data)-min(data))}
\end{align}

Dengan alpha merupakan weighted pada `min-max scaling`. Untuk variabel $X$ akan digunakan $\alpha=1$ sedangkan untuk variabel $R_t$ akan digunakan $\alpha=1.25$. Hal ini berarti hasil regresi memungkinkan $25\%$ lebih besar dari data historis dan $25\%$ lebih kecil dari nilai minimal data historis.
"""

from scipy.stats import norm
import matplotlib.pyplot as plt
def alpha(r):
  mu, std = norm.fit(r)
  # Generate points for the fitted distribution
  x = np.linspace(r.min(), r.max(), 100)
  pdf = norm.pdf(x, mu, std)

  confidence_level = 0.999
  a = 1 - confidence_level

  # Calculate the quantiles
  lower_quantile = norm.ppf(a / 2, loc=mu, scale=std)
  upper_quantile = norm.ppf(1 - a/ 2, loc=mu, scale=std)
  return lower_quantile, upper_quantile, pdf

target = 'Harga Timah Berjangka'
def DataGen(df,days):
  df["Tanggal"] = pd.to_datetime(df["Tanggal"])
  # Membagi data menjadi dua untuk variabel penjelas dan target
  y_list = []
  X_list = []
  for index, row in df.iterrows():
    target_date = row["Tanggal"] - timedelta(days=days)  # Target awal 30 hari sebelumnya

    # Cari tanggal dalam rentang 30-32 hari sebelumnya
    x_row = df[(df["Tanggal"] >= target_date) & (df["Tanggal"] <= target_date + timedelta(days=2))]
    if not x_row.empty:
      x_selected = x_row.iloc[-1]  # Pilih tanggal terbaru dalam rentang
      X_list.append(x_selected)
      y_list.append(row)  # Hanya menambahkan ke y jika ada pasangan di X

  # Konversi list ke DataFrame
  X = pd.DataFrame(X_list).reset_index(drop=True)
  y = pd.DataFrame(y_list)[["Tanggal", "Harga Timah Berjangka"]].reset_index(drop=True)

  # Feature engineering pada tanggal y
  y['Tanggal'] = pd.to_datetime(y['Tanggal'])
  #X['Tahun'] = y['Tanggal'].dt.year
  X['Bulan'] = y['Tanggal'].dt.month
  X['Hari'] = y['Tanggal'].dt.day

  #Membuat kolom rates: perubahan harga (persentase) pada waktu t+K
  rates_ = (y[target]-X[target])/X[target]
  lower, upper,pdf = alpha(rates_)

  #Membuat label status pakah pada saat t+K naik atau turun
  X['status']=rates_.apply(lambda x: 1 if x>0 else 0)

  #Menambahkan faktor alpha untuk penskalaan target (rates dan harga)
  y[target]=(y[target]-y[target].min())/((1+abs(upper))*y[target].max()-(1+lower)*y[target].min())
  X[f'Harga Timah {days}']=y[target]


  #Scaling pada rates
  X['rates']=(rates_-rates_.min())/((abs(upper)+1)*rates_.max()-rates_.min()*(1+abs(lower)))

  #Scaling variabel penjelas
  X[X.columns[1:-3]]=(X[X.columns[1:-3]]-X[X.columns[1:-3]].min())/(X[X.columns[1:-3]].max()-X[X.columns[1:-3]].min())
  return X[X.columns[1:]],y, rates_, pdf

X,y,r, pdf=DataGen(data_,30)
display(X.head())

pd.concat([X.head(2),X.sample(2),X.tail(2)])[X.columns[:-1]]

"""# **C. Exploratory Data Analysis**"""

# @title Default title text
def Corr(data,i):
  plt.figure(figsize=(10, 8))
  corr_=data.corr()
  mask = np.triu(np.ones_like(corr_, dtype='bool'),k=1)
  sns.heatmap(corr_,
      mask = mask,
      annot=True,                  # Menampilkan nilai korelasi
      fmt=".2f",                   # Format angka desimal
      cmap="coolwarm",             # Warna heatmap
      cbar=True,                   # Menampilkan color bar
      cbar_kws={"shrink": 0.8},    # Ukuran color bar
      linewidths=0.5,              # Lebar garis antar kotak
      linecolor=None,           # Warna garis antar kotak
      vmin=-1, vmax=1,             # Batas minimum dan maksimum nilai
      square=True,                 # Kotak berbentuk persegi
      xticklabels=True,            # Menampilkan label x
      yticklabels=True             # Menampilkan label y
  )

  # Tambahkan judul
  plt.title(f"Korelasi Hari-{i}", fontsize=16)

  # Tampilkan heatmap
  plt.show()

# Menghitung jumlah masing-masing kelas
def plot_and_balance_status(X):
  # Menampilkan distribusi awal
  counts = X['status'].value_counts()
  print("Distribusi Awal:")
  print(counts.to_frame().T)  # Tampilkan dalam format tabel menyamping

  # Pisahkan data berdasarkan kelas
  class_0 = X[X['status'] == 0]
  class_1 = X[X['status'] == 1]

  # Menentukan ukuran minimum dari kedua kelas
  min_size = min(len(class_0), len(class_1))

  # Potong data ke ukuran yang sama
  class_0_balanced = class_0.iloc[:min_size]
  class_1_balanced = class_1.iloc[:min_size]

  # Gabungkan data seimbang
  X_balanced = pd.concat([class_0_balanced, class_1_balanced])

  # Simpan kelebihan data dari kelas yang lebih besar
  if len(class_0) > len(class_1):
      X_excess = class_0.iloc[min_size:]
  else:
      X_excess = class_1.iloc[min_size:]

  # Menampilkan distribusi awal dan setelah penyeimbangan dalam satu baris
  fig, axes = plt.subplots(1, 2, figsize=(12, 5))

  # Plot distribusi awal
  counts.plot(kind='bar', color=['blue', 'orange'], alpha=0.7, ax=axes[0])
  axes[0].set_title('Distribusi Awal Kelas Status')
  axes[0].set_xlabel('Status')
  axes[0].set_ylabel('Jumlah')
  axes[0].set_xticks([0, 1])
  axes[0].set_xticklabels(['0', '1'], rotation=0)

  # Menampilkan distribusi setelah penyeimbangan
  balanced_counts = X_balanced['status'].value_counts()
  print("Distribusi Setelah Penyeimbangan:")
  print(balanced_counts.to_frame().T)  # Tampilkan dalam format tabel menyamping

  balanced_counts.plot(kind='bar', color=['blue', 'orange'], alpha=0.7, ax=axes[1])
  axes[1].set_title('Distribusi Setelah Penyeimbangan Kelas Status')
  axes[1].set_xlabel('Status')
  axes[1].set_ylabel('Jumlah')
  axes[1].set_xticks([0, 1])
  axes[1].set_xticklabels(['0', '1'], rotation=0)

  plt.tight_layout()
  plt.show()

  return X_balanced, X_excess

def PCA_model(X_train):
  mean = np.mean(X_train, axis=0)
  std_ = np.std(X_train, axis=0)
  df_std = (X_train-mean)/std_

  mat_cov = np.cov(df_std, rowvar=False)
  eigenvalues, vectoreigen = np.linalg.eig(mat_cov)
  index = np.argsort(eigenvalues)[::-1]
  eigenvalues_sorted = eigenvalues[index]
  vectoreigen_sorted = vectoreigen[:,index]
  Total = sum(eigenvalues_sorted)
  var_exp = [i*100/Total for i in eigenvalues_sorted]
  cum_var_exp = np.cumsum(var_exp)
  with plt.style.context('seaborn-v0_8-whitegrid'):
    plt.figure(figsize=(10, 6))

    plt.bar(range(1,len(X_train.columns)+1), var_exp, alpha=0.5, align='center',
            label='individual explained variance')
    plt.step(range(1,len(X_train.columns)+1), cum_var_exp, where='mid',
             label='cumulative explained variance')
    for i, core_val in enumerate(cum_var_exp):
      plt.annotate(str(round(cum_var_exp[i],1))+'%', (i+1, cum_var_exp[i]), xytext=(3, 6), textcoords='offset points',fontsize = 8)
    plt.ylabel('Explained variance ratio')
    plt.xlabel('Principal components')
    plt.legend(loc='best')
    plt.tight_layout()
  n_component=5
  vector_selected = vectoreigen_sorted[:,:n_component]
  result = df_std.dot(vector_selected)
  result.columns = [f'PCA Feature {i+1}' for i in range (n_component)]
  return result, vector_selected

def TransformPCA(X_data, vec_selected):
  X_mean = X_data.mean()
  X_std = X_data.std()
  X_norm = (X_data-X_mean)/X_std
  X_transform = X_norm.dot(vec_selected)
  X_transform.columns = [f'PCA Feature {i+1}' for i in range (len(X_transform.columns))]
  return X_transform #(X_transform-X_transform.min())/(X_transform.max()-X_transform.min())

def plot_rates(r,pdf):
  plt.figure(figsize=(10, 6))
  plt.hist(r, bins=30,edgecolor='black',density=True, alpha=0.6, label='Rate of Change')
  plt.plot(np.linspace(r.min(), r.max(), 100), pdf, 'r-', label='Fitted Normal Distribution')

  plt.xlabel('Rate of Change')
  plt.ylabel('Probability Density')
  plt.title('Normal Distribution Fit to Rate of Change')
  plt.legend()
  plt.show()

"""## **C.1. Data dengan $k=30$**"""

X_30,y_30, r , pdf=DataGen(data_,30)
display(X_30.head())

plot_rates(r,pdf)

X_balanced_30, X_excess_30=plot_and_balance_status(X_30)

sns.pairplot(X_30.iloc[:,:-2], hue='status')

Corr(X_balanced_30,30)

y_30=X_balanced_30['status']
  y_30_rates = X_balanced_30['rates']
  X_30=X_balanced_30.iloc[:,:len(X_balanced_30.columns)-3]
  X_train_30_, X_test_30_, y_train_30, y_test_30, y_train_30_rates,y_test_30_rates= train_test_split(X_30, y_30,y_30_rates, test_size=0.2, random_state=1)

X_train_30, vector_selected_30 = PCA_model(X_train_30_)

X_test_30 = TransformPCA(X_test_30_, vector_selected_30)
X_test_30.head(3)

"""## **C.2. Data sengan $k=60$**"""

X_60,y_60,r,pdf=DataGen(data_,60)

X_balanced_60, X_excess_30=plot_and_balance_status(X_60)

sns.pairplot(X_60.iloc[:,:-2], hue='status')

Corr(X_balanced_60,60)

y_60=X_balanced_60['status']
  y_60_rates = X_balanced_60['rates']
  X_60=X_balanced_60.iloc[:,:len(X_balanced_60.columns)-3]
  X_train_60_, X_test_60_, y_train_60, y_test_60, y_train_60_rates,y_test_60_rates = train_test_split(X_60, y_60,y_60_rates, test_size=0.2, random_state=42)

X_train_60, vector_selected_60 = PCA_model(X_train_60_)

X_test_60 = TransformPCA(X_test_60_, vector_selected_60)
X_test_60.head(3)

"""## **C.3. Data Dengan $k=90$**"""

X_90,y_90,r,pdf=DataGen(data_,90)
display(X_90.head())

X_balanced_90, X_excess_90=plot_and_balance_status(X_90)

sns.pairplot(X_90.iloc[:,:-2], hue='status')

Corr(X_balanced_90,90)

y_90=X_balanced_90['status']
  y_90_rates = X_balanced_90['rates']
  X_90=X_balanced_90.iloc[:,:len(X_balanced_90.columns)-3]
  X_train_90_, X_test_90_, y_train_90, y_test_90, y_train_90_rates, y_test_90_rates = train_test_split(X_90, y_90, y_90_rates, test_size=0.2, random_state=42)

X_train_90, vector_selected_90 = PCA_model(X_train_90_)

X_test_90 = TransformPCA(X_test_90_, vector_selected_90)
X_test_90.head(3)

"""# **D. Modeling Klasifikasi**"""

# @title Fingsi Fungsi untuk Modeling
# Model_Ann : Fungsi untuk modeling klasifikasi
#Model_Ann_re: Fungsi untuk modeling regresi
def Model_Ann(data,n_days):
  X_train, X_test, y_train, y_test = data
  #Modeling
  model = tf.keras.Sequential([
  tf.keras.layers.Input(shape=(len(X_train.columns),)),
  tf.keras.layers.Dense(16, activation='relu'),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(8, activation='relu'),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(1,activation='sigmoid') ])

  # Kompilasi model
  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

  # Latih model
  history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=150, batch_size=16)
  # Evaluasi model
  loss, mae = model.evaluate(X_test, y_test)
  y_pred = (model.predict(X_test) > 0.5).astype("int32")#model.predict(X_test)
  # Confusion Matrix
  cm = confusion_matrix(y_test, y_pred)
  # Classification Report
  cr = classification_report(y_test, y_pred)
  return model, history, X_test, y_test, cm, cr

####################################### Regresi #########################################
# Fungsi untuk membuat model ANN
def create_model_reg(X_train, neurons_layer1=10,neurons_layer2=10, dropout_rate=0.2):
  model = tf.keras.Sequential([
    tf.keras.layers.Input(shape=(X_train.shape[1],)),  # Input layer
    tf.keras.layers.Dense(neurons_layer1, activation='relu'),  # Hidden layer 1
    tf.keras.layers.Dropout(dropout_rate),              # Dropout layer
    tf.keras.layers.Dense(neurons_layer2, activation='relu'),  # Hidden layer 2
    tf.keras.layers.Dropout(dropout_rate),              # Dropout layer
    tf.keras.layers.Dense(1)                            # Output layer
  ])
  model.compile(optimizer='adam', loss='mse', metrics=['mae'])
  return model

def Model_Ann_reg(data,n_days):
  X_train, X_test, y_train, y_test = data
  # Grid Search
  # Definisikan parameter grid untuk Grid Search
  param_grid = {
      'neurons_layer1': [10, 20, 30],
      'neurons_layer2':[10, 16,20],# Jumlah neuron di hidden layers
      'dropout_rate': [0.2, 0.3, 0.4], # Dropout rate
      'batch_size': [16, 32, 64],             # Ukuran batch
      'epochs': [50, 100]                     # Jumlah epoch
  }

  # Kombinasi parameter
  param_combinations = list(product(param_grid['neurons_layer1'], param_grid['neurons_layer2'],
                                    param_grid['dropout_rate'], param_grid['batch_size'],
                                    param_grid['epochs']))

  # Grid Search Manual
  best_score = float('inf')
  best_params = None
  display(X_train.head())

  for neurons_layer1, neurons_layer2, dropout_rate, batch_size, epochs in param_combinations:
    model = create_model_reg(X_train, neurons_layer1, neurons_layer2, dropout_rate)
    history = model.fit(X_train, y_train, validation_data=(X_test, y_test),
                        epochs=epochs, batch_size=batch_size, verbose=0)
    y_pred = model.predict(X_test)
    mae = mean_absolute_error(y_test, y_pred)
    if mae < best_score:
      best_score = mae
      best_params = {'neurons_layer1': neurons_layer1, 'neurons_layer2': neurons_layer2,
                    'dropout_rate': dropout_rate, 'batch_size': batch_size, 'epochs': epochs}


  #Mode terbaik dari grid search
  model_opt = tf.keras.Sequential([
  tf.keras.layers.Input(shape=(len(X_train.columns),)),
  tf.keras.layers.Dense(best_params['neurons_layer1'], activation='relu'),
  tf.keras.layers.Dropout(best_params['dropout_rate']),
  tf.keras.layers.Dense(best_params['neurons_layer2'], activation='relu'),
  tf.keras.layers.Dropout(best_params['dropout_rate']),
  tf.keras.layers.Dense(1) ])

  # Kompilasi model
  model_opt.compile(optimizer='adam', loss='mse', metrics=['mae'])
  # Latih model
  history = model_opt.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=best_params['epochs'], batch_size=best_params['batch_size'])
  # Evaluasi model
  loss, mae = model_opt.evaluate(X_test, y_test)
  y_pred = model_opt.predict(X_test)
  mae = mean_absolute_error(y_test, y_pred)
  mse = mean_squared_error(y_test, y_pred)
  r2 = r2_score(y_test, y_pred)
  return model_opt, history, X_test, y_test, [mae, mse, r2]

def Model_Ann_reg_without_grid(data,n_days):
  X_train, X_test, y_train, y_test = data
  #Mode terbaik dari grid search
  model_opt = tf.keras.Sequential([
  tf.keras.layers.Input(shape=(len(X_train.columns),)),
  tf.keras.layers.Dense(16, activation='relu'),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(8, activation='relu'),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(1) ])

  # Kompilasi model
  model_opt.compile(optimizer='adam', loss='mse', metrics=['mae'])
  # Latih model
  history = model_opt.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=32)
  # Evaluasi model
  loss, mae = model_opt.evaluate(X_test, y_test)
  y_pred = model_opt.predict(X_test)
  mae = mean_absolute_error(y_test, y_pred)
  mse = mean_squared_error(y_test, y_pred)
  r2 = r2_score(y_test, y_pred)
  return model_opt, history,[mae, mse, r2]

################################################################################################
def ValidationPlot(history):
  loss = history.history['loss']
  val_loss = history.history['val_loss']
  accuracy = history.history.get('accuracy')  # Untuk klasifikasi
  val_accuracy = history.history.get('val_accuracy')

  epochs = range(1, len(loss) + 1)

  # Plot Loss
  plt.figure(figsize=(12, 5))

  plt.subplot(1, 2, 1)
  plt.plot(epochs, loss, 'bo-', label='Training Loss')
  plt.plot(epochs, val_loss, 'ro-', label='Validation Loss', linewidth=0.5)
  plt.title('Training and Validation Loss')
  plt.xlabel('Epochs')
  plt.ylabel('Loss')
  plt.legend()

  # Plot Accuracy
  if accuracy:
      plt.subplot(1, 2, 2)
      plt.plot(epochs, accuracy, 'bo-', label='Training Accuracy')
      plt.plot(epochs, val_accuracy, 'ro-', label='Validation Accuracy')
      plt.title('Training and Validation Accuracy')
      plt.xlabel('Epochs')
      plt.ylabel('Accuracy')
      plt.legend()

  plt.tight_layout()
  plt.show()

def plot_model(model, y_test,X_test, scale):
  y_pred = model.predict(X_test)
  mae = mean_absolute_error(y_test, y_pred)
  mse = mean_squared_error(y_test, y_pred)
  r2 = r2_score(y_test, y_pred)
  print(f"MAE: {mae:<.5f}, MSE: {mse:<.5f}, R²: {r2*100:<.2f}%")
  min_,max_, target=scale
  y_pred=pd.Series(np.array(y_pred)[:,0], index=y_test.index)
  y_pred_actual = y_pred*(max_[target]-min_[target])+min_[target]
  y_test_actual = y_test*(max_[target]-min_[target])+min_[target]
  plt.figure(figsize=(10,6))
  plt.plot(y_test_actual.values, label='Actual', color='blue', alpha=0.6)
  plt.plot(y_pred_actual.values, label='Predicted', color='red', linestyle='--', alpha=0.4)
  plt.title('Actual vs Predicted')
  plt.xlabel('Index')
  plt.ylabel('Value')
  plt.legend()
  plt.show()

def MonteCarloPrediction(model, input_, scale_, n_simulations=100):
  min_,max_,target= scale_
  min_target = min_[target]
  min_feat = min_.drop(index=target)
  max_target = max_[target]
  max_feat = max_.drop(index=target)
  Input = pd.Series(input_, index=min_feat.index)
  input_scale = (Input-min_feat)/(max_feat-min_feat)
  preds = np.array([model(input_scale.values.reshape(1,-1), training=True).numpy().flatten() for _ in range(n_simulations)])
  mean_preds = preds.mean(axis=0)
  #Statistics
  std_preds = preds.std(axis=0)
  cl_upper = mean_preds+1.96*std_preds
  cl_lower = mean_preds-1.96*std_preds
  #predict_output = model.predict(input_scale.values.reshape(1,-1))
  #predict_output_scale = predict_output*(max_target-min_target)+min_target
  return (mean_preds*(max_target-min_target)+min_target)[0], (cl_lower*(max_target-min_target)+min_target)[0], (cl_upper*(max_target-min_target)+min_target)[0]

"""## **D.1. Model Data +30 Hari**"""

model_30_ = Model_Ann([X_train_30_[X_train_30_.columns[1:3]],X_test_30_[X_train_30_.columns[1:3]],y_train_30,y_test_30], 30)

model_30 = Model_Ann([X_train_30,X_test_30,y_train_30,y_test_30], 30)

model_30[0].summary()

ValidationPlot(model_30_[1])

ValidationPlot(model_30[1])

"""**Confussion Matrix**"""

# Kelas
classes = ['Harga Turun (0)', 'Harga Naik (1)']

# Membuat Heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(model_30_[4], annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)

# Menambahkan label
plt.xlabel('Prediksi')
plt.ylabel('Aktual')
plt.title('Confusion Matrix Model Klasifikasi hari +30')
plt.show()

# Membuat Heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(model_30[4], annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)

# Menambahkan label
plt.xlabel('Prediksi')
plt.ylabel('Aktual')
plt.title('Confusion Matrix Model Klasifikasi hari +30')
plt.show()

"""Dari hasil di atas diperoleh
- Hasil prediksi benar bahwa dari 281 data akan menyebabkan harga timah pada hari +30 akan turun
- Kemudian 98 data diprediksi harga timah pada hari +30 akan naik padahal sebenarnya turun
- Hasil prediksi benar bahwa 255 data akan menyebabkan harga timah pada hari +30 akan naik
- Sedangkan prediksi 92 data akan menyebabkan harga timah pada hari +30 akan turun padahal harga timah naik.

**Metrik Evaluasi**

Dari confusion matrix ini, kita bisa menghitung metrik evaluasi seperti berikut:

1. Accuracy: Proporsi prediksi yang benar terhadap semua prediksi.
  \begin{align}
    Accuracy &=\dfrac{TN+TP}{TN+FP+FN+TP} \\
    Accuracy&=\dfrac{281+82+92+255}{281+255}​​≈0.74
  \end{align}
  

2. Precision (kelas 1): Proporsi prediksi harga naik yang benar terhadap semua prediksi harga naik.
  \begin{align}
    Precision&=\dfrac{TP}{TP+FP} \\
    Precision&=\dfrac{255}{82+255}​=0.72
  \end{align}
3. Recall (kelas 1): Proporsi harga naik yang berhasil terprediksi dengan benar terhadap semua data harga naik.
  \begin{align}
    Recall&=\dfrac{TP}{TP+FN} \\
    Recall&=\dfrac{281}{92+255}​​≈0.73
  \end{align}
4. F1-Score (kelas 1): Rata-rata harmonis antara Precision dan Recall.
  \begin{align}
    F1-Score&&=2⋅\dfrac{Precision⋅Recall}{Precision+Recall} \\
    F1-Score&&=2\dfrac{0.72\cdot 0.73 }{0.72+0.73}​≈0.73
  \end{align}

Dengan menggunakan code diperoleh hasil sebagai berikut
"""

print(model_30_[5])

print(model_30[5])

"""Berdasarkan hasil tersebut
- Model memiliki akurasi $74\%$, artinya $74\%$ dari prediksi model sesuai dengan nilai aktual.
- Precision $72\%$ berarti hanya $72\%$ dari prediksi harga naik benar-benar naik.
- Recall $73\%$ menunjukkan bahwa dari semua harga yang benar-benar naik, $732\%$ berhasil terprediksi dengan benar.
- F1-Score $73\%$ menunjukkan keseimbangan antara Precision dan Recall.

## **D.2. Model Data +60 Hari**
"""

model_60 = Model_Ann([X_train_60,X_test_60,y_train_60,y_test_60], 60)

ValidationPlot(model_60[1])

"""**Confussion Matrix**"""

# Kelas
classes = ['Harga Turun (0)', 'Harga Naik (1)']

# Membuat Heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(model_60[4], annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)

# Menambahkan label
plt.xlabel('Prediksi')
plt.ylabel('Aktual')
plt.title('Confusion Matrix Model Klasifikasi hari +30')
plt.show()

"""Dari hasil di atas diperoleh
- Hasil prediksi benar bahwa dari 263 data akan menyebabkan harga timah pada hari +30 akan turun
- Kemudian 116 data diprediksi harga timah pada hari +30 akan naik padahal sebenarnya turun
- Hasil prediksi benar bahwa 289 data akan menyebabkan harga timah pada hari +30 akan naik
- Sedangkan prediksi 64 data akan menyebabkan harga timah pada hari +30 akan turun padahal harga timah naik.

**Metrik Evaluasi**

Dari confusion matrix ini, kita bisa menghitung metrik evaluasi seperti berikut:

1. Accuracy: Proporsi prediksi yang benar terhadap semua prediksi.
  \begin{align}
    Accuracy &=\dfrac{TN+TP}{TN+FP+FN+TP} \\
    Accuracy&=\dfrac{263+116+64+289}{263+289}​​≈0.75
  \end{align}
  

2. Precision (kelas 1): Proporsi prediksi harga naik yang benar terhadap semua prediksi harga naik.
  \begin{align}
    Precision&=\dfrac{TP}{TP+FP} \\
    Precision&=\dfrac{289}{116+289}​≈0.71
  \end{align}
3. Recall (kelas 1): Proporsi harga naik yang berhasil terprediksi dengan benar terhadap semua data harga naik.
  \begin{align}
    Recall&=\dfrac{TP}{TP+FN} \\
    Recall&=\dfrac{289}{64+289}​​≈0.82
  \end{align}
4. F1-Score (kelas 1): Rata-rata harmonis antara Precision dan Recall.
  \begin{align}
    F1-Score&&=2⋅\dfrac{Precision⋅Recall}{Precision+Recall} \\
    F1-Score&&=2\dfrac{0.71\cdot 0.82 }{0.71+0.82}​≈0.76
  \end{align}

Dengan menggunakan code diperoleh hasil sebagai berikut
"""

print(model_60[5])

"""Berdasarkan hasil tersebut
- Model memiliki akurasi $74\%$, artinya $74\%$ dari prediksi model sesuai dengan nilai aktual.
- Precision $72\%$ berarti hanya $72\%$ dari prediksi harga naik benar-benar naik.
- Recall $73\%$ menunjukkan bahwa dari semua harga yang benar-benar naik, $73\%$ berhasil terprediksi dengan benar.
- F1-Score $73\%$ menunjukkan keseimbangan antara Precision dan Recall.

## **D.3. Model Data +90 Hari**
"""

model_90 = Model_Ann([X_train_90,X_test_90,y_train_90,y_test_90], 90)

model_90 = Model_Ann([X_train_90,X_test_90,y_train_90,y_test_90], 90)

ValidationPlot(model_90[1])

"""**Confussion Matrix**"""

# Kelas
classes = ['Harga Turun (0)', 'Harga Naik (1)']

# Membuat Heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(model_90[4], annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)

# Menambahkan label
plt.xlabel('Prediksi')
plt.ylabel('Aktual')
plt.title('Confusion Matrix Model Klasifikasi hari +30')
plt.show()

"""Dari hasil di atas diperoleh
- Hasil prediksi benar bahwa dari 308 data akan menyebabkan harga timah pada hari +30 akan turun
- Kemudian 57 data diprediksi harga timah pada hari +30 akan naik padahal sebenarnya turun
- Hasil prediksi benar bahwa 231 data akan menyebabkan harga timah pada hari +30 akan naik
- Sedangkan prediksi 90 data akan menyebabkan harga timah pada hari +30 akan turun padahal harga timah naik.

**Metrik Evaluasi**

Dari confusion matrix ini, kita bisa menghitung metrik evaluasi seperti berikut:

1. Accuracy: Proporsi prediksi yang benar terhadap semua prediksi.
  \begin{align}
    Accuracy &=\dfrac{TN+TP}{TN+FP+FN+TP} \\
    Accuracy&=\dfrac{308+57+90+231}{308+231}​=0.79
  \end{align}
  

2. Precision (kelas 1): Proporsi prediksi harga naik yang benar terhadap semua prediksi harga naik.
  \begin{align}
    Precision&=\dfrac{TP}{TP+FP} \\
    Precision&=\dfrac{231}{57+231}​=407261​≈0.8
  \end{align}
3. Recall (kelas 1): Proporsi harga naik yang berhasil terprediksi dengan benar terhadap semua data harga naik.
  \begin{align}
    Recall&=\dfrac{TP}{TP+FN} \\
    Recall&=\dfrac{231}{90+231}​​≈0.72
  \end{align}
4. F1-Score (kelas 1): Rata-rata harmonis antara Precision dan Recall.
  \begin{align}
    F1-Score&&=2⋅\dfrac{Precision⋅Recall}{Precision+Recall} \\
    F1-Score&&=2\dfrac{0.8\cdot 0.72 }{0.8+0.72}​≈0.76
  \end{align}

Dengan menggunakan code diperoleh hasil sebagai berikut
"""

print(model_90[5])

"""Berdasarkan hasil tersebut
- Model memiliki akurasi $79\%$, artinya $79\%$ dari prediksi model sesuai dengan nilai aktual.
- Precision $80\%$ berarti hanya $80\%$ dari prediksi harga naik benar-benar naik.
- Recall $72\%$ menunjukkan bahwa dari semua harga yang benar-benar naik, $72\%$ berhasil terprediksi dengan benar.
- F1-Score $74\%$ menunjukkan keseimbangan antara Precision dan Recall.
"""

model_30_reg = Model_Ann_reg_without_grid([X_train_30_,X_test_30_,y_train_30_rates,y_test_30_rates], 30)

"""# **E. Modeling For Regresi**

Saya juga mencoba memodelkan harga timah menggunakan regresi. Ini merupakan versi sebelumnya dari klasifikasi yang saya coba
"""

# @title Fungsi
def DataGen_(data,days):
  min_, max_ = data['Tanggal'].min(), data['Tanggal'].max()
  y=data[data['Tanggal']>min_+timedelta(days=days)].iloc[:,[0,4,5,6,7]]
  X=data.iloc[len(data)-len(y):,:].reset_index(drop=True).iloc[:,[0,1,2,3]]
  #X_offset=y['Tanggal']-pd.DateOffset(days=n_days)
 # X=X[X['Tanggal'].isin(X_offset)].reset_index(drop=True)
 # y=y.iloc[:len(X)]
  return pd.concat([X.iloc[:,1:], y.iloc[:,1:]], axis=1)

def Model_Ann_(data,n_days):
  #Pengambilan data validasi
  data['Tahun']=data.Tanggal.dt.year
  data['Bulan']=data.Tanggal.dt.month
  data['Hari']=data.Tanggal.dt.day

  data_validation = data.iloc[:n_days+3,:]
  data = data.drop(index=data_validation.index).reset_index(drop=True)

  Data = DataGen_(data,n_days)
  target = 'Harga Timah Berjangka'
  #Scalling Data
  data_min = Data.min()
  data_max = Data.max()
  data_scaled = (Data-data_min)/(data_max-data_min)

  #Data For modeling
  y=data_scaled[target]
  X=data_scaled.drop(columns=[target])
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

  #Modeling
  model = tf.keras.Sequential([
  tf.keras.layers.Input(shape=(len(X.columns),)),      # 4 variabel penjelas
  tf.keras.layers.Dense(16, activation='relu'),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(8, activation='relu'),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(1) ])               # 1 output untuk prediks

# Kompilasi model
  model.compile(optimizer='adam', loss='mse', metrics=['mae'])

  # Latih model
  #model.fit(X_train, y_train, epochs=50, batch_size=8, validation_split=0.2, verbose=1)
  history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=32)
  # Evaluasi model
  loss, mae = model.evaluate(X_test, y_test)
  y_pred = model.predict(X_test)
  mae = mean_absolute_error(y_test, y_pred)
  mse = mean_squared_error(y_test, y_pred)
  r2 = r2_score(y_test, y_pred)
  return model, history, X_test, y_test , [data_min, data_max, target],  [mae, mse, r2], data_validation

"""## **E.1. Modeling Percobaan**"""

model_ = Model_Ann_(data_ , 30)

ValidationPlot(model_[1])

plot_model(model_[0], model_[3], model_[2], model_[4])

"""Dari hasil di atas, hasil prediksi cukup bagus dengan $R^2$ mencapai $80$ persen.

## **E.2. Simulasi Untuk day=30, 60, dan 90**
"""

#Simulasi untuk lama prediksi
day_length=[30 , 60, 90]
Res = []
Model_ =[]
for i,t in enumerate(day_length):
  print(t)
  data = data_.copy()
  model_ = Model_Ann(data, t)
  Model_.append(model_[:2])
  scale = model_[4]
  error_ = model_[5]
  data_validation = model_[-1]
  data_actual = data_validation.iloc[:3,:]
  data_predictor = data_validation.iloc[-3:,:]
  y_actual= data_actual[model_[4][-1]].values
  y_actual_as_predictor = data_predictor[model_[4][-1]].values
  X_predictor = data_predictor.drop(columns=['Tanggal',model_[4][-1]]).to_numpy()
  tanggal_prediksi = data_actual.Tanggal.values
  tanggal_prediktor = data_predictor.Tanggal.values
  res_day = []
  for j in range (len(data_actual)):
    mean_ , lw, up = MonteCarloPrediction(model_[0], X_predictor[j], scale)
    res_day.append([tanggal_prediksi[j],tanggal_prediktor[j],y_actual[j],y_actual_as_predictor[j]]+[mean_, lw, up]+error_)
  Res.append(res_day)

for i,res_ in enumerate(Res):
  print(f'Hail Validasi untuk {day_length[i]} Hari kedepan')
  display(pd.DataFrame(res_, columns=['Tanggal Prediksi','Tanggal Prediktor','Aktual','Prediktor','Hail Prediksi','Lower 95%', 'Upper 95%', 'MAE', 'MSE', 'R^2']))

"""Pada hasil di atas berikut validasi yang diambil dari tiga sampel.
- Pada prediksi 30 hari ke depan, semua prediksi benar, harga timah pada `2024-10-04` sebesar $33805$ sedangkan prediksi pada `2024-11-15` sebesar $31275$. atau harga timah akan turun, sedangkan aktualnya $28742$ atau harga timah memang turun. Untuk kedua hasil yang lainnya juga demikian.
- Pada prediksi 60 hari ke depan, hasil prediksi sudah tepat
- Pada prediksi 90 hari kedepan, semua hasil prediksi benar.

## **E.3.3 Input User**
"""

data_[data_['Tanggal']==datetime(2024,10,4)]

"""Misal input User Yang digunakan yaitu pada tanggal `4 Oktober 2024` untuk memprediksi pada tanggal `15 November 2024`
- Nilai Tukar: 15480
- Harga Minyak Mentah: 74.38
- Harga Batu Bara : 148.2
"""

model = Model_Ann_(data_ , 30)

# @title Input User
Tahun = int(input("Masukkan Tahun Prediksi (Dalam angka): "))
Bulan = int(input("Masukkn Bulan Prediksi( Dalam angka): "))
Hari = int(input("Masukkan Hari Prediksi( Dalam angka): "))
Cur = float(input("Masukkan Nilai Tukar H-30: "))
Oil = float(input("Masukkan Harga Benchmark Minyak H-30: "))
Coal=float(input("Masukkan Harga Benchmark Batu Bara H-30: "))
#Timah = float(input("masukkan Harga Timah H-30: "))
df_min, df_max , target=data_.iloc[:,1:].min(), data_.iloc[:,1:].max(), 'Harga Timah Berjangka'
B = [31,28,31,30,31,30,31,31,30,31,30,31]
if Bulan>12 or type(Bulan)=='float':
  cond =False
else:
  if Hari>B[Bulan-1] or type(Bulan)=='float':
    cond =False
  else:
    cond =True
while cond== False:
  print('--------------- Tolong Masukkan Input dengan Benar-----------------')
  Tahun = int(input("Masukkan Tahun Prediksi (Dalam angka): "))
  Bulan = int(input("Masukkn Bulan Prediksi( Dalam angka): "))
  Hari = int(input("Masukkan Hari Prediksi( Dalam angka): "))
  Cur = float(input("MasukkanNilai Tukar H-30: "))
  Oil = float(input("Masukkan Harga Benchmark Minyak H-30: "))
  Coal=float(input("Masukkan Harga Benchmark Batu Bara H-30: "))
  #Timah = float(input("masukkan Harga Timah H-30: "))
  if Bulan>12 or type(Bulan)=='float':
    cond =False
  else:
    if Hari>B[Bulan-1] or type(Bulan)=='float':
      cond =False
    else:
      cond =True
      print(f'--------------Konfirmasi Data yang di Inpuu---------------- \nTanggal Prediksi {Hari}-{Bulan}-{Tahun}  \nNilai Tukar Rupiah (USD/IDR) H-30 {Cur}  \nHarga Minyak {Oil} USD/Barel \nHarga Batu Bara {Coal}')
      conf = str(input('Apakah Inputan Anda Sudah Tepat ? (y/n): '))
      if conf in ['y','Y','yes','ya','Yes','Ya']:
        cond_ =True
      else:
        cond_ = False
      while cond_ ==False:
        print('--------------- Tolong Masukkan Input dengan Benar-----------------')
        Tahun = int(input("Masukkan Tahun Prediksi (Dalam angka): "))
        Bulan = int(input("Masukkn Bulan Prediksi( Dalam angka): "))
        Hari = int(input("Masukkan Hari Prediksi( Dalam angka): "))
        Cur = float(input("Masukkan Nilai Tukar H-30: "))
        Oil = float(input("Masukkan Harga Benchmark Minyak H-30: "))
        Coal=float(input("Masukkan Harga Benchmark Batu Bara H-30: "))
        #Timah = float(input("masukkan Harga Timah H-30: "))
        if conf in ['y','Y','yes','ya','Yes','Ya']:
          cond_ =True
          input_ =[Cur,Oil, Coal,Tahun, Bulan, Hari]
          scale_ = [df_min, df_max, target]
          res=MonteCarloPrediction(model[0], input_, scale_)
          print(f'Hasil prediksi harga timah pada {Hari}-{Bulan}-{Tahun} yaitu {res[0]:<.3f} \nDengan selang kepercayaan $90%$ \nHarga minimal {res[1]:<.3f}\nHarga maksimal {res[2]:<.3f}')
        else:
          cond_ = False
      else:
          input_ =[Cur,Oil, Coal,Tahun, Bulan, Hari]
          scale_ = [df_min, df_max, target]
          res=MonteCarloPrediction(model[0], input_, scale_)
          print(f'Hasil prediksi harga timah pada {Hari}-{Bulan}-{Tahun} yaitu {res[0]:<.3f} \nDengan selang kepercayaan $90%$ \nHarga minimal {res[1]:<.3f}\nHarga maksimal {res[2]:<.3f}')
else:
  print(f'--------------Konfirmasi Data yang di Inpuu---------------- \nTanggal Prediksi {Hari}-{Bulan}-{Tahun}  \nNilai Tukar Rupiah (USD/IDR) H-30 {Cur}  \nHarga Minyak {Oil} USD/Barel \nHarga Batu Bara {Coal}')
  conf = str(input('Apakah Inputan Anda Sudah Tepat ? (y/n): '))
  if conf in ['y','Y','yes','ya','Yes','Ya']:
    cond_ =True
  else:
    cond_ = False
  while cond_ ==False:
    print('--------------- Tolong Masukkan Input dengan Benar-----------------')
    Tahun = int(input("Masukkan Tahun Prediksi (Dalam angka): "))
    Bulan = int(input("Masukkn Bulan Prediksi( Dalam angka): "))
    Hari = int(input("Masukkan Hari Prediksi( Dalam angka): "))
    Cur = float(input("Masukkan Nilai Tukar H-30: "))
    Oil = float(input("Masukkan Harga Benchmark Minyak H-30: "))
    Coal=float(input("Masukkan Harga Benchmark Batu Bara H-30: "))
    #Timah = float(input("masukkanHarga Timah H-30: "))
    if conf in ['y','Y','yes','ya','Yes','Ya']:
      cond_ =True
      input_ =[Cur,Oil, Coal,Tahun, Bulan, Hari]
      scale_ = [df_min, df_max, target]
      res=MonteCarloPrediction(model[0], input_, scale_)
      print(f'Hasil prediksi harga timah pada {Hari}-{Bulan}-{Tahun} yaitu {res[0]:<.3f} \nDengan selang kepercayaan $90%$ \nHarga minimal {res[1]:<.3f}\nHarga maksimal {res[2]:<.3f}')
    else:
      cond_ = False
  else:
    input_ =[Cur,Oil, Coal, Tahun, Bulan, Hari]
    scale_ = [df_min, df_max, target]
    res=MonteCarloPrediction(model[0], input_, scale_)
    print(f'Hasil prediksi harga timah pada {Hari}-{Bulan}-{Tahun} yaitu {res[0]:<.3f} \nDengan selang kepercayaan $90%$ \nHarga minimal {res[1]:<.3f}\nHarga maksimal {res[2]:<.3f}')

"""# **F. Simpulan**

Hasil modeling dari klasifikasi dan regresi menggunakan ANN sudah cukup baik meskipun hasilnya mungkin bisa dioptimalkan lagi. Mengingat waktu yang terbatas, penulis masih belum bisa mengeksplor lebih jauh. Namun penulis memiliki sebuah ide untuk meningkatkan hasilnya yaitu dengan mengombinasikan data preprosesing. Seperti `currency` yang langsung dimasukan ke faktor harga setiap komoditas yang digunakan. Selain itu, terdapat dua hari liburdalam satu minggu dalam mengenerate index harga. Sehingga perlu disesuaikan dan bisa juga dilakukan interpolasi untuk data yang tidak ada pada perdgangan. Meskipun idealnya lebih ke penyesuaian. Ukuran data juga terlalu kecil hanya sekitar 3800 baris sehingga kemungkinan masih belum bisa menyerap variabilitas dari populasi yang sebenarnya.
"""